{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22143,"status":"ok","timestamp":1747590051433,"user":{"displayName":"Jaroji Jaroji","userId":"10588697490620152236"},"user_tz":-180},"id":"3MTH-gzkJou1","outputId":"9acf0e9c-f53d-4e79-cc35-c8ecc5d9f641"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","CrowdCounting directory found at: /content/drive/MyDrive/CrowdCounting\n","Current working directory changed to: /content/drive/MyDrive/CrowdCounting\n"]}],"source":["from google.colab import drive\n","import os\n","\n","drive.mount('/content/drive')\n","\n","# Adjust the path if it's located in a subfolder\n","project_path = \"/content/drive/MyDrive/CrowdCounting\"\n","\n","# Check if the path exists\n","if os.path.exists(project_path):\n","  print(f\"CrowdCounting directory found at: {project_path}\")\n","  # Change the working directory to CrowdCounting\n","  os.chdir(project_path)\n","  print(f\"Current working directory changed to: {os.getcwd()}\")\n","else:\n","  print(f\"Error: CrowdCounting directory not found at {project_path}\")\n","  print(\"Please ensure the path is correct and the directory exists.\")\n"]},{"cell_type":"code","source":["# unmount drive in case the modified files couldn't updated to notebook then run mount again\n","drive.flush_and_unmount()\n","print('All changes made in this colab session should now be visible in Drive.')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9GaVaI6ANMhm","executionInfo":{"status":"ok","timestamp":1747582058093,"user_tz":-180,"elapsed":3058,"user":{"displayName":"Jaroji Jaroji","userId":"10588697490620152236"}},"outputId":"a701f97d-6c9e-4938-a48d-6727bf5ba4e6"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["All changes made in this colab session should now be visible in Drive.\n"]}]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30905,"status":"ok","timestamp":1747591087795,"user":{"displayName":"Jaroji Jaroji","userId":"10588697490620152236"},"user_tz":-180},"id":"9KWi7KPQHepp","outputId":"6bd2f678-30ff-4b6d-d1f5-7daa842723e2"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["Skipping weight copy for: 0.weight due to shape mismatch\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating: 100%|██████████| 34/34 [00:20<00:00,  1.66it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","== Evaluation Metrics ==\n","MAE   : 54.429\n","MSE   : 5795.169\n","RMSE  : 76.126\n","MNAE  : 0.114\n","PSNR  : 27.017 dB\n","SSIM  : 0.7648\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["# import all library needed\n","import json\n","import torch\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from tqdm import tqdm\n","\n","from model import CSRNet\n","import importlib\n","import dataset  # pertama kali import\n","importlib.reload(dataset)\n","from dataset import listDataset\n","\n","from skimage.metrics import structural_similarity as ssim\n","from skimage.metrics import peak_signal_noise_ratio as psnr\n","import cv2\n","\n","# configurations\n","TRAIN_JSON       = 'test_data.json'         # define test set\n","CHECKPOINT_PATH  = 'hajj2-mask1-model_best.pth.tar' # define model that will evaluated\n","DEVICE           = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') # define gpu number\n","BATCH_SIZE       = 1                          # define batch size, keep it 1 due to different images dimension\n","NUM_WORKERS      = 2                          # number of worker for dataloader, keep it 2\n","MAX_BATCHES      = None                       # set None to conduct full evaluation\n","\n","# define function for data loader\n","def get_dataloader(json_path, batch_size, num_workers):\n","    with open(json_path, 'r') as f:\n","        img_list = json.load(f)\n","\n","    transform = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                             std=[0.229, 0.224, 0.225])\n","    ])\n","\n","    dataset = listDataset(img_list, transform=transform, train=False)\n","    loader = DataLoader(\n","        dataset,\n","        batch_size=batch_size,\n","        shuffle=False,\n","        num_workers=num_workers,\n","        pin_memory=True\n","    )\n","    return loader\n","\n","train_loader = get_dataloader(TRAIN_JSON, BATCH_SIZE, NUM_WORKERS)\n","\n","# define function for load model\n","def load_model(checkpoint_path, device):\n","    model = CSRNet().to(device)\n","    ckpt  = torch.load(checkpoint_path, map_location=device)\n","    model.load_state_dict(ckpt['state_dict'])\n","    model.eval()\n","    return model\n","\n","# define object for the model\n","model = load_model(CHECKPOINT_PATH, DEVICE)\n","\n","# define function to compute all metrics are needed\n","def evaluate_all_metrics(loader, model, device, max_batches=None):\n","    mae_sum, mse_sum, mnae_sum = 0.0, 0.0, 0.0\n","    psnr_list, ssim_list = [], []\n","    n = 0\n","\n","    for i, (imgs, targets) in enumerate(tqdm(loader, desc='Evaluating')):\n","        if max_batches is not None and i >= max_batches:\n","            break\n","        imgs, targets = imgs.to(device), targets.to(device)\n","        with torch.no_grad():\n","            preds = model(imgs)\n","\n","        pred_cnts = preds.sum(dim=(1,2,3)).cpu().numpy()\n","        true_cnts = targets.sum(dim=(1,2)).cpu().numpy()\n","\n","        mae_sum  += np.abs(pred_cnts - true_cnts).sum()\n","        mse_sum  += ((np.abs(pred_cnts - true_cnts))**2).sum()\n","        mnae_sum += (np.abs(pred_cnts - true_cnts) / (true_cnts + 1e-8)).sum()\n","\n","        for b in range(imgs.size(0)):\n","            pred_map = preds[b].squeeze().cpu().numpy()\n","            gt_map = targets[b].squeeze().cpu().numpy()\n","            # Resize pred_map to gt_map size\n","            pred_map_resized = cv2.resize(pred_map, (gt_map.shape[1], gt_map.shape[0]), interpolation=cv2.INTER_CUBIC)\n","            psnr_val = psnr(gt_map, pred_map_resized, data_range=gt_map.max() - gt_map.min())\n","            ssim_val = ssim(gt_map, pred_map_resized, data_range=gt_map.max() - gt_map.min())\n","            psnr_list.append(psnr_val)\n","            ssim_list.append(ssim_val)\n","\n","        n += imgs.size(0)\n","\n","    mae  = mae_sum / n\n","    mse  = mse_sum / n\n","    rmse = np.sqrt(mse_sum / n)\n","    mnae = mnae_sum / n\n","    avg_psnr = np.mean(psnr_list)\n","    avg_ssim = np.mean(ssim_list)\n","\n","    return mae, mse, rmse, mnae, avg_psnr, avg_ssim\n","\n","mae, mse, rmse, mnae, avg_psnr, avg_ssim = evaluate_all_metrics(train_loader, model, DEVICE, MAX_BATCHES)\n","\n","print(f\"\\n== Evaluation Metrics ==\")\n","print(f\"MAE   : {mae:.3f}\")\n","print(f\"MSE   : {mse:.3f}\")\n","print(f\"RMSE  : {rmse:.3f}\")\n","print(f\"MNAE  : {mnae:.3f}\")\n","print(f\"PSNR  : {avg_psnr:.3f} dB\")\n","print(f\"SSIM  : {avg_ssim:.4f}\")"]},{"cell_type":"markdown","source":["## SHAP implementation\n","We need make some adjustment to use shap for interpreting crowd counting model. It because basically shap to interpret classification output"],"metadata":{"id":"gacpU6Mk4CX4"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","# Wrap model to return total count\n","class CSRNetWrapper(nn.Module):\n","    def __init__(self, model):\n","        super(CSRNetWrapper, self).__init__()\n","        self.model = model\n","\n","    def forward(self, x):\n","        density_map = self.model(x)   # (1, 1, H, W)\n","        count = density_map.view(x.size(0), -1).sum(dim=1, keepdim=True) # (1, 1)\n","\n","        return count\n","\n","# set the relu to false due to shap didn't support relu.\n","def disable_inplace_relu(module):\n","    for name, child in module.named_children():\n","        if isinstance(child, nn.ReLU):\n","            setattr(module, name, nn.ReLU(inplace=False))\n","        else:\n","            disable_inplace_relu(child)\n","\n","# Patch all inplace ReLU\n","disable_inplace_relu(model)\n","# Wrapped model\n","wrapped_model = CSRNetWrapper(model)\n","# Set model to eval mode\n","wrapped_model.eval()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7kFIekgxeP6a","executionInfo":{"status":"ok","timestamp":1747591087828,"user_tz":-180,"elapsed":14,"user":{"displayName":"Jaroji Jaroji","userId":"10588697490620152236"}},"outputId":"4e3db340-c79f-4ad1-e668-0354fe298e3f"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["CSRNetWrapper(\n","  (model): CSRNet(\n","    (frontend): Sequential(\n","      (0): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU()\n","      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (3): ReLU()\n","      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (6): ReLU()\n","      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (8): ReLU()\n","      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (11): ReLU()\n","      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (13): ReLU()\n","      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (15): ReLU()\n","      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (18): ReLU()\n","      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (20): ReLU()\n","      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (22): ReLU()\n","    )\n","    (backend): Sequential(\n","      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n","      (1): ReLU()\n","      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n","      (3): ReLU()\n","      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n","      (5): ReLU()\n","      (6): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n","      (7): ReLU()\n","      (8): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n","      (9): ReLU()\n","      (10): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n","      (11): ReLU()\n","    )\n","    (output_layer): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n","  )\n",")"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["from torchvision import transforms\n","from torch.utils.data import DataLoader\n","import json\n","\n","# Since SHAP needs same dimension size for all images, we force dimension the input size\n","def get_dataloader(json_path, batch_size=1, num_workers=0, resize=(512, 512)):\n","    with open(json_path, 'r') as f:\n","        img_list = json.load(f)\n","\n","    transform = transforms.Compose([\n","        transforms.ToTensor(),  # ubah dari np.ndarray ke torch.Tensor\n","        #transforms.Resize((512, 512)),  # Resize tensor\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    ])\n","\n","\n","    dataset = listDataset(img_list, transform=transform, train=False)\n","\n","    loader = DataLoader(\n","        dataset,\n","        batch_size=batch_size,\n","        shuffle=False,\n","        num_workers=num_workers,\n","        pin_memory=True\n","    )\n","    return loader\n"],"metadata":{"id":"FlU-feDSLbPe","executionInfo":{"status":"ok","timestamp":1747591087842,"user_tz":-180,"elapsed":13,"user":{"displayName":"Jaroji Jaroji","userId":"10588697490620152236"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Define function for denormalize image\n","def denormalize(img_tensor,*, mean, std):\n","    mean = torch.tensor(mean).view(-1, 1, 1)\n","    std = torch.tensor(std).view(-1, 1, 1)\n","    return img_tensor * std + mean\n","\n","# Since SHAP needs same dimension size for all images, we force the dimension to same face\n","def resize_tensor_image(tensor_img, size=(512, 512)):\n","    return F.interpolate(tensor_img, size=size, mode='bilinear', align_corners=False)\n","\n","# Define function to plot original image\n","def image_rgb(image_tensor,idx):\n","\n","    img_denorm = denormalize(image_tensor[0, :3].cpu(), mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    image_np = np.transpose(img_denorm.numpy(), (1, 2, 0))  # (C, H, W) → (H, W, C)\n","\n","    plt.figure(figsize=(10, 4))\n","    plt.imshow(image_np)\n","    plt.title(f'Sample #{idx}')\n","    plt.axis('off')\n","    plt.show()\n","\n","# Define function to visualize SHAP value\n","def visualize_shap_channel(image_tensor, shap_value_tensor, idx, channel_idx, channel_name):\n","\n","    channel_input = image_tensor[0, channel_idx].detach().cpu().numpy()\n","    channel_shap  = shap_value_tensor[0, channel_idx]\n","\n","    vmax = max(np.max(np.abs(channel_shap)), 1e-3)  # batas minimum untuk kontras\n","\n","\n","    plt.figure(figsize=(10, 4))\n","\n","    plt.subplot(1, 2, 1)\n","    if channel_name == \"Edge\":\n","      plt.imshow(channel_input, cmap='gray')\n","    else:\n","      plt.imshow(channel_input,cmap=channel_name)\n","    plt.title(f'Sample #{idx} - {channel_name} Channel Input')\n","    plt.axis('off')\n","    plt.subplot(1, 2, 2)\n","    im = plt.imshow(channel_shap, cmap='seismic', vmin=-vmax, vmax=vmax)\n","    plt.title(f'Sample #{idx} - SHAP for {channel_name}')\n","    plt.colorbar(im, fraction=0.046, pad=0.04)\n","    plt.tight_layout()\n","    plt.axis('off')\n","    plt.show()\n","\n","# Define function to visualize SHAP feature important per chanel (SHAP mean)\n","def plot_shap_bar(avg_shaps, channel_names, idx):\n","    plt.figure(figsize=(6, 4))\n","    plt.bar(channel_names, avg_shaps, color='skyblue')\n","    plt.title(f\"Sample #{idx} - Avg |SHAP| per Channel\")\n","    plt.ylabel(\"Mean(|SHAP|)\")\n","    plt.tight_layout()\n","    plt.grid(True, axis='y')\n","    plt.show()\n","\n","# Define function to plot gt dan pred density map\n","def plot_density(gt, pred, idx):\n","    # 1. Ground Truth Density Map\n","    plt.figure(figsize=(10, 5))\n","    plt.subplot(1, 2, 1)\n","    plt.imshow(gt.squeeze(), cmap='jet')\n","    plt.title(f\"GT Density Map\\nCount: {gt.sum():.1f}\")\n","    plt.axis(\"off\")\n","\n","    # 2. Predicted Density Map\n","    plt.subplot(1, 2, 2)\n","    plt.imshow(pred.squeeze(), cmap='jet')\n","    plt.title(f\"Predicted Map\\nCount: {pred.sum():.1f}\")\n","    plt.axis(\"off\")\n","    plt.show()"],"metadata":{"id":"Y8woZPqR20PI","executionInfo":{"status":"ok","timestamp":1747591087865,"user_tz":-180,"elapsed":21,"user":{"displayName":"Jaroji Jaroji","userId":"10588697490620152236"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# import all library are needed\n","import shap\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","from torchvision import transforms\n","import torchvision.transforms.functional as TF\n","import torch.nn.functional as F\n","\n","# Initialize the image loader\n","shap_loader = get_dataloader(TRAIN_JSON, batch_size=1, num_workers=0)\n","shap_iter = iter(shap_loader)\n","\n","# Name of channle\n","channel_names = [\"Reds\", \"Greens\", \"Blues\", \"Edge\"]\n","\n","# define list to save mae value ratio, this is needed for get the best and worst case\n","mae_ratios = []\n","\n","# define list to save background for shap\n","background_list = []\n","\n","# run looping for all test set images\n","for idx, (image, target) in enumerate(shap_iter):\n","    #image, target = next(shap_iter)\n","    image_resized = resize_tensor_image(image, size=(512, 512)).cpu() # forcing image size\n","    # take 5 images for shap background\n","    if len(background_list) < 5:\n","        background_list.append(image_resized.clone())\n","    # predicting original images (not resized)\n","    with torch.no_grad():\n","        output = model(image.to(DEVICE))\n","        pred = output.cpu().numpy()\n","        true = target.cpu().numpy()\n","        pred_cnts = output.sum(dim=(1,2,3)).cpu().numpy()\n","        true_cnts = target.sum(dim=(1,2)).cpu().numpy()\n","        abs_diff = np.abs(pred_cnts - true_cnts)[0]\n","        gt = true_cnts[0]\n","\n","        # Avoid devide by zero\n","        ratio = abs_diff / (gt + 1e-8)\n","\n","    # Save information needed for next step to list\n","    mae_ratios.append({\n","        'idx': idx,\n","        'ratio': ratio,\n","        'mae': abs_diff,\n","        'gt': gt,\n","        'pred': pred_cnts[0],\n","        'd_gt': true,\n","        'd_pred': pred,\n","        'image': image,\n","        'image_resized': image_resized\n","    })\n","\n","# Take sample for the best and worst case\n","min_ratio_sample = min(mae_ratios, key=lambda x: x['ratio'])\n","max_ratio_sample = max(mae_ratios, key=lambda x: x['ratio'])\n","\n","\n","# save the best and worst value to list\n","samples_to_show = [min_ratio_sample, max_ratio_sample]\n","# save background for shap analysis\n","background = torch.cat(background_list, dim=0)\n"],"metadata":{"id":"569jVQoG-QK1","executionInfo":{"status":"ok","timestamp":1747591090212,"user_tz":-180,"elapsed":1721,"user":{"displayName":"Jaroji Jaroji","userId":"10588697490620152236"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Loop for those cases\n","for sample in samples_to_show:\n","    # get all information are needed from the list\n","    idx = sample['idx']\n","    image = sample['image']\n","    image_resized = sample['image_resized']\n","    gt = sample['gt']\n","    pred = sample['pred']\n","    ratio = sample['ratio']\n","    mae_val = sample['mae']\n","    gt_density = sample['d_gt']\n","    pred_density = sample['d_pred']\n","\n","    # Run SHAP\n","    wrapped_model = wrapped_model.cpu()                       # Used this model to align shap requirement\n","    wrapped_model.eval()                                      # set model to evaluation mode\n","    explainer = shap.DeepExplainer(wrapped_model, background) # define explainer, we use deep explainer\n","    shap_values = explainer.shap_values(image_resized)        # get shape values for best and worst value\n","\n","    # Plot the rgp image\n","    image_rgb(image, idx)\n","\n","    # Plot all density (gt and pred)\n","    plot_density(gt_density, pred_density, idx)\n","\n","    # Print all informations about the sample (GT, PRED, MAE and Ratio)\n","    print(f\"\\nSample #{idx} - GT: {gt:.2f}, Pred: {pred:.2f}, MAE: {mae_val:.2f}, Ratio: {ratio:.4f}\")\n","\n","    # Shap visualization per channel\n","    avg_shaps = []\n","    for ch in range(4):\n","        max_val = np.abs(shap_values[0][0, ch]).max()\n","        mean_val = np.abs(shap_values[0][0, ch]).mean()\n","        avg_shaps.append(mean_val)\n","        # print shap (mean) value per channel\n","        print(f\"Channel {ch} ({channel_names[ch]}): max SHAP = {max_val:.6f}, mean SHAP = {mean_val:.6f}\")\n","        # plot image per channel\n","        visualize_shap_channel(image, shap_values[0], idx=idx, channel_idx=ch, channel_name=channel_names[ch])\n","\n","    # bar plot for shap (feature importance per channel)\n","    plot_shap_bar(avg_shaps, channel_names, idx)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1E8-ed5935tAZOkKGnMb3LPZxPR9EQcf3"},"id":"iAOGRjLefFI2","executionInfo":{"status":"ok","timestamp":1747591133955,"user_tz":-180,"elapsed":42068,"user":{"displayName":"Jaroji Jaroji","userId":"10588697490620152236"}},"outputId":"ff75340a-9c1b-4d73-9304-e0691b1c41d4"},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","provenance":[],"authorship_tag":"ABX9TyM0tdSdO25KdxRp3Wz7XVrN"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}